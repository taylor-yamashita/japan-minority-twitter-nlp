{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# tokenization\n",
    "import json\n",
    "import MeCab\n",
    "import import_ipynb\n",
    "import thesis_preprocess\n",
    "from stopwords.stopwords_ja import stop_words\n",
    "from stopwords.stopwords_slothlib import stop_words_2\n",
    "\n",
    "# lda topic modeling\n",
    "import gensim, logging\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lda.readthedocs.io/en/latest/getting_started.html\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "# https://github.com/deankuo/Japan-Manifesto-Classification/blob/main/topic_modeling.ipynb\n",
    "# https://github.com/m3yrin/NTM/blob/master/LDA_jp.ipynb\n",
    "# https://tdual.hatenablog.com/entry/2018/04/09/133000#1LDA%E3%81%AE%E5%89%8D%E3%81%AB%E3%83%88%E3%83%94%E3%83%83%E3%82%AF%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A8%E3%81%AF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize cleaned tweets into words\n",
    "def tokenize(text):\n",
    "    mt = MeCab.Tagger(\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "    parsed = mt.parseToNode(text)\n",
    "    components = []\n",
    "    \n",
    "    while parsed:\n",
    "        word = parsed.surface\n",
    "        pos = parsed.feature.split(\",\")[0]\n",
    "\n",
    "        # for lda, we only want nouns, verbs, adjectives\n",
    "        include_pos = [\"名詞\", \"動詞\", \"形容詞\", \"副詞\"]\n",
    "        if pos in include_pos: components.append(word)\n",
    "        parsed = parsed.next\n",
    "    \n",
    "    # remove stopwords\n",
    "    components = [token for token in components if ((not token in stop_words) and (not token in stop_words_2))]\n",
    "    \n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run preprocessing and tokenization for tweets from given .txt file\n",
    "def preprocess_tokenize_all(filename, year):\n",
    "    # store results and exception tweets\n",
    "    tokens = []\n",
    "    retweets = []\n",
    "    not_parsed = []\n",
    "\n",
    "    # iterate through tweets, preprocess and tokenize\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            if line == None or tweet == None:\n",
    "                not_parsed.append((line, tweet))\n",
    "                print(\"Parsing error: \", line, tweet)\n",
    "            elif tweet['retweetedTweet']:\n",
    "                retweets.append(tweet)\n",
    "                print(\"Retweet: \", tweet['id'])\n",
    "            # filter out 2024 sponsored(?) tweets\n",
    "            elif int(tweet['date'].split(\"-\")[0]) < int(year) + 1: \n",
    "                tweet_text = tweet['rawContent'] # note: need other prop for over 140 char?\n",
    "                processed = thesis_preprocess.preprocess(tweet_text)            \n",
    "                components = tokenize(processed)\n",
    "                tokens.append(components)\n",
    "\n",
    "    file.close()\n",
    "    return tokens, retweets, not_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in filename you want to save data as, including '.csv'\n",
    "def save_to_csv(tweet_tokens, filename):\n",
    "    f = open(filename, 'w')\n",
    "    writer = csv.writer(f)\n",
    "    for tweet in tweet_tokens:\n",
    "        writer.writerow(tweet)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in filename of csv you want to load, including '.csv'\n",
    "def load_from_csv(filename):\n",
    "    with open(filename, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        tweet_tokens = list(reader)\n",
    "    return tweet_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and save lda model for given year; data_name is id suffix to save lda model\n",
    "def run_lda(data_name, tweet_tokens, num_topics=10, no_below=5, no_above=0.6, save=True):\n",
    "    # set up dictionary\n",
    "    dict = corpora.Dictionary(tweet_tokens)\n",
    "    dict.filter_extremes(no_below, no_above)\n",
    "    dict.compactify()\n",
    "\n",
    "    # set up corpus\n",
    "    corpus = [dict.doc2bow(w) for w in tweet_tokens]\n",
    "    test_size = int(len(corpus) * 0.1)\n",
    "    test_corpus = corpus[:test_size]\n",
    "    train_corpus = corpus[test_size:]   \n",
    "\n",
    "    # train and save lda model\n",
    "    logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                          id2word=dict, \n",
    "                                          num_topics=num_topics, \n",
    "                                          random_state=100, \n",
    "                                          passes=10, \n",
    "                                          update_every=3, \n",
    "                                          alpha='auto',\n",
    "                                          per_word_topics=True)\n",
    "    if save: lda.save(\"saved_lda_models/lda_model_\" + data_name)\n",
    "    \n",
    "    return lda, dict, corpus, train_corpus, test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display words comprising topics and proportion\n",
    "def examine_topics(lda, dict, num_topics=10):\n",
    "    for topic in range(num_topics):\n",
    "        print(\"Topic # \",(topic+1))\n",
    "        for t in lda.get_topic_terms(topic):\n",
    "            print(\"{}: {}\".format(dict[t[0]], t[1]))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display just the words comprising topics\n",
    "def examine_topics_plain(lda, dict, num_topics=10):\n",
    "    for topic in range(num_topics):\n",
    "        print(\"Topic # \",(topic+1))\n",
    "        for t in lda.get_topic_terms(topic):\n",
    "            print(dict[t[0]])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare train/test perplexity\n",
    "def analyze_train_test_results(lda, train_corpus, test_corpus):\n",
    "    # look at train set results\n",
    "    N = sum(count for doc in train_corpus for _, count in doc)\n",
    "    print(\"# of words in train corpus: \",N)\n",
    "    perplexity = np.exp2(-lda.log_perplexity(train_corpus))\n",
    "    print(\"perplexity(train):\", perplexity,\"\\n\")\n",
    "\n",
    "    # look at test set results\n",
    "    N = sum(count for doc in test_corpus for _, count in doc)\n",
    "    print(\"# of words in test corpus: \",N)\n",
    "    perplexity = np.exp2(-lda.log_perplexity(test_corpus))\n",
    "    print(\"perplexity(test):\", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at overall perplexity and coherence score\n",
    "def analyze_overall_results(lda, tweet_tokens, dict, corpus):\n",
    "    print('\\nPerplexity: ', lda.log_perplexity(corpus))     # lower is better\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(model=lda, texts=tweet_tokens, dictionary=dict, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)     # higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open interactive visualization of topics\n",
    "def visualize_topics(lda, corpus, dict):\n",
    "    pyLDAvis.enable_notebook(local=True)\n",
    "    vis = pyLDAvis.gensim.prepare(lda, corpus, dict)\n",
    "    pyLDAvis.show(vis, local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for _, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = pd.concat([sent_topics_df, pd.DataFrame([[int(topic_num), round(prop_topic,4), topic_keywords]], columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords'])], ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "def get_dominant_topics(df_topic_sents_keywords):\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    df_dominant_topic.head(10)\n",
    "    return df_topic_sents_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group top 5 sentences under each topic\n",
    "def get_representative_docs(df_topic_sents_keywords):\n",
    "    sent_topics_sorteddf = pd.DataFrame()\n",
    "    sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "    for _, grp in sent_topics_outdf_grpd:\n",
    "        sent_topics_sorteddf = pd.concat([sent_topics_sorteddf, \n",
    "                                                grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                                axis=0)\n",
    "    # reset index    \n",
    "    sent_topics_sorteddf.reset_index(drop=True, inplace=True)\n",
    "    # format and show\n",
    "    sent_topics_sorteddf.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "    sent_topics_sorteddf.head()\n",
    "    return sent_topics_sorteddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show further details about topics\n",
    "def run_topic_analysis(df_topic_sents_keywords):\n",
    "    # num documents per topic\n",
    "    topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "    # percentage documents for each topic\n",
    "    topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "    # topic number, keywords\n",
    "    topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "    # concatenate column-wise\n",
    "    df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "    # add column names\n",
    "    df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "    # show\n",
    "    df_dominant_topics\n",
    "\n",
    "    return df_dominant_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining good number of topics for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run lda and compute coherence for range of numbers of topics\n",
    "def compute_coherence_values(tokens, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        # model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        lda, dict, _, _, _ = run_lda(\"\", tokens, num_topics=num_topics, save=False)\n",
    "        model_list.append(lda)\n",
    "        coherencemodel = CoherenceModel(model=lda, texts=tokens, dictionary=dict, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display coherence values (highest is best)\n",
    "def show_coherence_vals(coherence_values, limit=40, start=2, step=6):\n",
    "    x = range(start, limit, step)\n",
    "    for m, cv in zip(x, coherence_values):\n",
    "        print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot coherence vals for different # topics to find best num_topics\n",
    "def plot_coherence_vals(coherence_values, limit=40, start=2, step=6):\n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-hw276cHk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
