{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "import json\n",
    "import MeCab\n",
    "import demoji\n",
    "import mojimoji\n",
    "import re\n",
    "from stopwords_ja import stop_words\n",
    "from stopwords_slothlib import stop_words_2\n",
    "\n",
    "# lda topic modelling\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import corpora\n",
    "# import gensim.corpora as corpora\n",
    "# from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess tweet content\n",
    "def preprocess(text):    \n",
    "    # from https://colab.research.google.com/drive/1bX-JyY4xmCm_RFkJg3QNcthUvEJaBghP\n",
    "    # handle half-width/full-width chars, jp punctuation\n",
    "    text = text.lower()\n",
    "    text = mojimoji.zen_to_han(text, kana=False)\n",
    "    text = mojimoji.han_to_zen(text, digit=False, ascii=False)\n",
    "    text = text.translate(str.maketrans({\n",
    "        '!': '！', '\"': '”', '#': '＃', '$': '＄', '%': '％', '&': '＆', '\\'': '’',\n",
    "        '(': '（', ')': '）', '*': '＊', '+': '＋', ',': '，', '-': '−', '.': '．',\n",
    "        '/': '／', ':': '：', ';': '；', '<': '＜', '=': '＝', '>': '＞', '?': '？',\n",
    "        '@': '＠', '[': '［', '\\\\': '＼', ']': '］', '^': '＾', '_': '＿', '`': '｀',\n",
    "        '{': '｛', '|': '｜', '}': '｝'\n",
    "        }))\n",
    "    zenkaku_leftsingle = b'\\xe2\\x80\\x98'.decode('utf-8')\n",
    "    text = re.sub('[’´｀]', zenkaku_leftsingle, text)\n",
    "    \n",
    "    # remove twitter-specific strings (handles, hashtags, etc.)\n",
    "    text = re.sub(\"@([a-zA-Z0-9_]+)\", \"\", text)\n",
    "    text = re.sub(\"#([a-zA-Z0-9_ぁ-んァ-ン一-龠]+)\", \"\", text)\n",
    "    text = re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"\", text)\n",
    "\n",
    "    # remove emojis\n",
    "    text = demoji.replace(text, \"\")\n",
    "    text = re.sub(\"([\\uD83E-\\uD83E])+\", \"\", text)\n",
    "\n",
    "    # remove punctuation and whitespace\n",
    "    text = re.sub(\"([^一-龯ぁ-んァ-ン])+\",\"\",text)  \n",
    "    text = re.sub(\"(\\s)+\", \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize cleaned tweets into words\n",
    "def tokenize(text):\n",
    "    mt = MeCab.Tagger(\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "    parsed = mt.parseToNode(text)\n",
    "    components = []\n",
    "    \n",
    "    while parsed:\n",
    "        word = parsed.surface\n",
    "        pos = parsed.feature.split(\",\")[0]\n",
    "\n",
    "        # for lda, we only want nouns, verbs, adjectives\n",
    "        include_pos = [\"名詞\", \"動詞\", \"形容詞\"]\n",
    "        if pos in include_pos: components.append(word)\n",
    "        parsed = parsed.next\n",
    "    \n",
    "    # remove stopwords\n",
    "    components = [token for token in components if ((not token in stop_words) and (not token in stop_words_2))]\n",
    "    \n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run preprocessing and tokenization for all tweets from given year dataset\n",
    "def preprocess_tokenize_all(year):\n",
    "    # store results and exception tweets\n",
    "    tokens = []\n",
    "    retweets = []\n",
    "    not_parsed = []\n",
    "\n",
    "    # iterate through tweets, preprocess and tokenize\n",
    "    with open(year + '-all.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            if line == None or tweet == None:\n",
    "                not_parsed.append((line, tweet))\n",
    "                print(\"Parsing error: \", line, tweet)\n",
    "            elif tweet['retweetedTweet']:\n",
    "                retweets.append(tweet)\n",
    "                print(\"Retweet: \", tweet['id'])\n",
    "            else: \n",
    "                tweet_text = tweet['rawContent']\n",
    "                # preprocess text\n",
    "                processed = preprocess(tweet_text)            \n",
    "                # tokenize with mecab\n",
    "                components = tokenize(processed)\n",
    "                tokens.append(components)\n",
    "\n",
    "    file.close()\n",
    "    return tokens, retweets, not_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run for 2015\n",
    "# tokens_2015, retweets_2015, not_parsed_2015 = preprocess_tokenize_all(\"2015\")\n",
    "\n",
    "# # did we get retweets or errors?\n",
    "# print(len(retweets_2015))\n",
    "# print(len(not_parsed_2015))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for 2022\n",
    "tokens_2022, retweets_2022, not_parsed_2022 = preprocess_tokenize_all(\"2022\")\n",
    "\n",
    "# did we get retweets or errors?\n",
    "print(len(retweets_2022))\n",
    "print(len(not_parsed_2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"thesis_lda_2022_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tokens_2022 = np.load(\"thesis_lda_2022_tweets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-hw276cHk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
