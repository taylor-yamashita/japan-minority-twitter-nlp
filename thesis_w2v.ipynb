{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "import json\n",
    "import MeCab\n",
    "import import_ipynb\n",
    "import thesis_preprocess\n",
    "from stopwords.stopwords_ja import stop_words\n",
    "from stopwords.stopwords_slothlib import stop_words_2\n",
    "\n",
    "# word2vec\n",
    "import gensim, logging\n",
    "\n",
    "# plotting\n",
    "from sklearn.manifold import TSNE               \n",
    "import numpy as np                \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and Tokenize Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize cleaned tweets into words\n",
    "def tokenize(text):\n",
    "    mt = MeCab.Tagger(\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "    parsed = mt.parseToNode(text)\n",
    "    components = []\n",
    "    \n",
    "    while parsed:\n",
    "        word = parsed.surface\n",
    "        pos = parsed.feature.split(\",\")[0]\n",
    "\n",
    "        # remove beg/end tokens, particles, fillers, auxiliary bound prefixes/endings\n",
    "        exclude_pos = ['BOS/EOS', '助詞', 'フィラー', '接頭詞', '助動詞']\n",
    "        if pos not in exclude_pos: components.append(word)\n",
    "        parsed = parsed.next\n",
    "    \n",
    "    # remove stopwords\n",
    "    components = [token for token in components if ((not token in stop_words) and (not token in stop_words_2))]\n",
    "    \n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_22 = thesis_preprocess.get_unique_tweets(\"datasets_general_years/2022-all.txt\", 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_15 = thesis_preprocess.get_unique_tweets(\"datasets_general_years/2015-all.txt\", 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run preprocessing and tokenization for tweets from given .txt file\n",
    "def preprocess_tokenize_all_unique(filename, year):\n",
    "    tokens = []\n",
    "    tweets = thesis_preprocess.get_unique_tweets(filename, year)\n",
    "    for tweet in tweets:\n",
    "        processed = thesis_preprocess.preprocess(tweet)            \n",
    "        components = tokenize(processed)\n",
    "        tokens.append(components)\n",
    "\n",
    "    return tokens, tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_15_2 = preprocess_tokenize_all_2(\"datasets_general_years/2015-all.txt\",\"2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run preprocessing and tokenization for all tweets from given year dataset\n",
    "def preprocess_tokenize_all(year):\n",
    "    # store results and exception tweets\n",
    "    tokens = []\n",
    "    retweets = []\n",
    "    not_parsed = []\n",
    "\n",
    "    # iterate through tweets, preprocess and tokenize\n",
    "    with open('datasets_general_years/'+ year + '-all.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            if line == None or tweet == None:\n",
    "                not_parsed.append((line, tweet))\n",
    "                print(\"Parsing error: \", line, tweet)\n",
    "            elif tweet['retweetedTweet']:\n",
    "                retweets.append(tweet)\n",
    "                print(\"Retweet: \", tweet['id'])\n",
    "            # filter out 2024 sponsored(?) tweets\n",
    "            elif int(tweet['date'].split(\"-\")[0]) < int(year) + 1: \n",
    "                tweet_text = tweet['rawContent'] # note: need other prop for over 140 char?\n",
    "                processed = thesis_preprocess.preprocess(tweet_text)            \n",
    "                components = tokenize(processed)\n",
    "                tokens.append(components)\n",
    "\n",
    "    file.close()\n",
    "    return tokens, retweets, not_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for 2015\n",
    "tokens_2015, retweets_2015, not_parsed_2015 = preprocess_tokenize_all(\"2015\")\n",
    "\n",
    "# did we get retweets or errors?\n",
    "print(len(retweets_2015))\n",
    "print(len(not_parsed_2015))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for 2022\n",
    "tokens_2022, retweets_2022, not_parsed_2022 = preprocess_tokenize_all(\"2022\")\n",
    "\n",
    "# did we get retweets or errors?\n",
    "print(len(retweets_2022))\n",
    "print(len(not_parsed_2022))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and save word2vec model for given year\n",
    "def run_word2vec(year, tokens):\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = gensim.models.Word2Vec(tokens, min_count=5)\n",
    "    model.save(\"saved_w2v_models/w2v_model_\" + year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_word2vec(\"2015\", tokens_15_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and save word2vec model for 2015 \n",
    "run_word2vec(\"2015\", tokens_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and save word2vec model for 2022\n",
    "run_word2vec(\"2022\", tokens_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained word2vec model\n",
    "model_2015 = gensim.models.Word2Vec.load(\"saved_w2v_models/w2v_model_2015\")\n",
    "\n",
    "# check similarity given by trained model\n",
    "print(model_2015.wv.most_similar(positive='在日',topn=10))\n",
    "print(model_2015.wv.most_similar(positive='外国人',topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for 2022\n",
    "model_2022 = gensim.models.Word2Vec.load(\"saved_w2v_models/w2v_model_2022\")\n",
    "\n",
    "print(model_2022.wv.most_similar(positive='在日',topn=10))\n",
    "print(model_2022.wv.most_similar(positive='外国人',topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Word2Vec Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2\n",
    "\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot similar words\n",
    "# https://aneesha.medium.com/using-tsne-to-plot-a-subset-of-similar-words-from-word2vec-bb8eeaea6229 \n",
    "# https://albertauyeung.github.io/2020/03/15/matplotlib-cjk-fonts.html/\n",
    "# https://stackoverflow.com/questions/70268270/how-to-plot-tsne-on-word2vec-created-from-gensim-for-the-most-similar-20-cases\n",
    "\n",
    "def plot_closest_words(word, model, x_vals, y_vals):\n",
    "    labels = [i for i in model.wv.index_to_key]\n",
    "    close_words = [i[0] for i in model.wv.most_similar(positive=word, topn=15)]\n",
    "\n",
    "    fprop = fm.FontProperties(fname='NotoSansJP-VariableFont_wght.ttf')\n",
    "    for word in close_words:\n",
    "        i = labels.index(word)\n",
    "        plt.scatter(x_vals[i],y_vals[i])\n",
    "        plt.annotate(labels[i], xy=(x_vals[i], y_vals[i]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom', fontproperties=fprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals, y_vals = reduce_dimensions(model_2015)\n",
    "plot_closest_words(\"外人\", model_2015, x_vals, y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals, y_vals = reduce_dimensions(model_2022)\n",
    "plot_closest_words(\"外人\", model_2022, x_vals, y_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2V Similar Words - Minority Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_similar_words(keyword:str, model_2015, model_2022, positive=[], negative=[], topn=10):\n",
    "    if len(positive) == 0: positive = keyword\n",
    "\n",
    "    # 2015\n",
    "    print(\"\\nSimilar words to \" + keyword + \": 2015\")\n",
    "    try:\n",
    "        words_15 = model_2015.wv.most_similar(positive=positive, negative=negative, topn=topn)\n",
    "        for w in words_15:\n",
    "            print(w[0])\n",
    "    except:\n",
    "        print(\"Error\\n\")\n",
    "\n",
    "    # 2022\n",
    "    print(\"\\nSimilar words to \" + keyword + \": 2022\")\n",
    "    try:\n",
    "        words_22 = model_2022.wv.most_similar(positive=positive, negative=negative, topn=topn)\n",
    "        for w in words_22:\n",
    "            print(w[0])\n",
    "    except:\n",
    "        print(\"Error\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2015 = gensim.models.Word2Vec.load(\"saved_w2v_models/w2v_model_2015\")\n",
    "model_2022 = gensim.models.Word2Vec.load(\"saved_w2v_models/w2v_model_2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2015 = gensim.models.Word2Vec.load(\"saved_w2v_models/w2v_model_2015\")\n",
    "model_2022 = gensim.models.Word2Vec.load(\"saved_w2v_models/w2v_model_2022\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zainichi Koreans\n",
    "compare_similar_words(\"在日\", model_2015, model_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ainu\n",
    "compare_similar_words(\"アイヌ\", model_2015, model_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_similar_words(\"沖縄\", model_2015, model_2022, positive=[\"沖縄\",\"日本人\"])\n",
    "compare_similar_words(\"琉球\", model_2015, model_2022, positive=[\"琉球\",\"日本人\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_similar_words(\"ハフ\", model_2015, model_2022, positive=[\"ハフ\",\"日本人\"\n",
    "                                                              ], negative=[\"髪\",\"服\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_similar_words(\"\", model_2015, model_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_similar_words(\"フィリピン\", model_2015, model_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_similar_words(\"外人\", model_2015, model_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_similar_words(\"外国人\", model_2015, model_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(keyword:str, model, positive=[], negative=[], topn=10):\n",
    "    if len(positive) == 0: positive = keyword\n",
    "\n",
    "    print(\"\\nSimilar words to \" + keyword + \": 2015\")\n",
    "    try:\n",
    "        words = model.wv.most_similar(positive=positive, negative=negative, topn=topn)\n",
    "        for w in words:\n",
    "            print(w[0])\n",
    "    except:\n",
    "        print(\"Error\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_words(\"在日\", model_2015)\n",
    "get_similar_words(\"アイヌ\", model_2015)\n",
    "get_similar_words(\"沖縄\", model_2015, positive=[\"沖縄\",\"日本人\"])\n",
    "get_similar_words(\"琉球\", model_2015, positive=[\"沖縄\",\"日本人\"])\n",
    "get_similar_words(\"ハフ\", model_2015, positive=[\"ハフ\",'日本人'], negative=[\"髪\",\"服\"])\n",
    "get_similar_words(\"ベトナム\", model_2015)\n",
    "get_similar_words(\"フィリピン\", model_2015)\n",
    "get_similar_words(\"外人\", model_2015)\n",
    "get_similar_words(\"外国人\", model_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
