{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "import json\n",
    "import MeCab\n",
    "import import_ipynb\n",
    "import thesis_preprocess\n",
    "from stopwords.stopwords_ja import stop_words\n",
    "from stopwords.stopwords_slothlib import stop_words_2\n",
    "\n",
    "# word2vec\n",
    "import gensim, logging\n",
    "\n",
    "# plotting\n",
    "from sklearn.manifold import TSNE               \n",
    "import numpy as np                \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Tokenize Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize cleaned tweets into words\n",
    "def tokenize_w2v(text):\n",
    "    mt = MeCab.Tagger(\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "    parsed = mt.parseToNode(text)\n",
    "    components = []\n",
    "    \n",
    "    while parsed:\n",
    "        word = parsed.surface\n",
    "        pos = parsed.feature.split(\",\")[0]\n",
    "\n",
    "        # remove beg/end tokens, particles, fillers, auxiliary bound prefixes/endings\n",
    "        exclude_pos = ['BOS/EOS', '助詞', 'フィラー', '接頭詞', '助動詞']\n",
    "        if pos not in exclude_pos: components.append(word)\n",
    "        parsed = parsed.next\n",
    "    \n",
    "    # remove stopwords\n",
    "    components = [token for token in components if ((not token in stop_words) and (not token in stop_words_2))]\n",
    "    \n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and tokenize with w2v-specific tokenize function\n",
    "def preprocess_tokenize_all_unique(filename, year):\n",
    "    tokens = []\n",
    "    tweets = thesis_preprocess.get_unique_tweets(filename, year)\n",
    "    for tweet in tweets:\n",
    "        processed = thesis_preprocess.preprocess(tweet)            \n",
    "        components = tokenize_w2v(processed)\n",
    "        tokens.append(components)\n",
    "\n",
    "    return tokens, tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_15, tweets_15 = preprocess_tokenize_all_unique(\"datasets_general_years/2015-all.txt\",\"2015\")\n",
    "thesis_preprocess.save_to_csv(tokens_15,\"saved_tokens/2015-all.csv\")\n",
    "thesis_preprocess.save_to_csv(tweets_15,\"saved_tweets/2015-all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_22, tweets_22 = preprocess_tokenize_all_unique(\"datasets_general_years/2022-all.txt\",\"2022\")\n",
    "thesis_preprocess.save_to_csv(tokens_22,\"saved_tokens/2022-all.csv\")\n",
    "thesis_preprocess.save_to_csv(tweets_22,\"saved_tweets/2022-all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and save word2vec model for given year\n",
    "def run_word2vec(year, tokens):\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = gensim.models.Word2Vec(tokens, min_count=5)\n",
    "    model.save(\"saved_w2v_models_unique/w2v_model_\" + year)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and save word2vec model for 2015 \n",
    "model_2015 = run_word2vec(\"2015\", tokens_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and save word2vec model for 2022\n",
    "model_2022 = run_word2vec(\"2022\", tokens_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(keyword:str, model, positive=[], negative=[], topn=10):\n",
    "    if len(positive) == 0: positive = keyword\n",
    "\n",
    "    print(\"\\nSimilar words to \" + keyword + \": 2015\")\n",
    "    try:\n",
    "        words = model.wv.most_similar(positive=positive, negative=negative, topn=topn)\n",
    "        for w in words:\n",
    "            print(w[0])\n",
    "    except:\n",
    "        print(\"Error\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_similar_words(keyword:str, model_2015, model_2022, positive=[], negative=[], topn=10):\n",
    "    if len(positive) == 0: positive = keyword\n",
    "\n",
    "    # 2015\n",
    "    print(\"\\nSimilar words to \" + keyword + \": 2015\")\n",
    "    try:\n",
    "        words_15 = model_2015.wv.most_similar(positive=positive, negative=negative, topn=topn)\n",
    "        for w in words_15:\n",
    "            print(w[0])\n",
    "    except:\n",
    "        print(\"Error\\n\")\n",
    "\n",
    "    # 2022\n",
    "    print(\"\\nSimilar words to \" + keyword + \": 2022\")\n",
    "    try:\n",
    "        words_22 = model_2022.wv.most_similar(positive=positive, negative=negative, topn=topn)\n",
    "        for w in words_22:\n",
    "            print(w[0])\n",
    "    except:\n",
    "        print(\"Error\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2015 = gensim.models.Word2Vec.load(\"saved_w2v_models_unique/w2v_model_2015\")\n",
    "model_2022 = gensim.models.Word2Vec.load(\"saved_w2v_models_unique/w2v_model_2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_words(\"在日\", model_2015)\n",
    "get_similar_words(\"アイヌ\", model_2015)\n",
    "get_similar_words(\"沖縄\", model_2015, positive=[\"沖縄\",\"日本人\"])\n",
    "get_similar_words(\"琉球\", model_2015, positive=[\"琉球\",\"日本人\"])\n",
    "get_similar_words(\"ハフ\", model_2015, positive=[\"ハフ\",'日本人'], negative=[\"髪\",\"服\"])\n",
    "get_similar_words(\"ベトナム\", model_2015)\n",
    "get_similar_words(\"フィリピン\", model_2015)\n",
    "get_similar_words(\"外人\", model_2015)\n",
    "get_similar_words(\"外国人\", model_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_similar_words(\"在日\", model_2015, model_2022)\n",
    "compare_similar_words(\"アイヌ\", model_2015, model_2022)\n",
    "compare_similar_words(\"沖縄\", model_2015, model_2022, positive=[\"沖縄\",\"日本人\"])\n",
    "compare_similar_words(\"琉球\", model_2015, model_2022, positive=[\"琉球\",\"日本人\"])\n",
    "compare_similar_words(\"ハフ\", model_2015, model_2022, positive=[\"ハフ\",\"日本人\"], negative=[\"髪\",\"服\"])\n",
    "compare_similar_words(\"ベトナム\", model_2015, model_2022)\n",
    "compare_similar_words(\"フィリピン\", model_2015, model_2022)\n",
    "compare_similar_words(\"外人\", model_2015, model_2022)\n",
    "compare_similar_words(\"外国人\", model_2015, model_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
