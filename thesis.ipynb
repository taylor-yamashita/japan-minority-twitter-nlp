{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import json\n",
    "import MeCab\n",
    "import demoji\n",
    "import re\n",
    "from stop_words import stop_words\n",
    "import gensim, logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and Tokenize Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize with mecab\n",
    "mt = MeCab.Tagger(\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "# dataset we are working with\n",
    "year = \"2015\"\n",
    "\n",
    "# store results and exception tweets\n",
    "tokens = []\n",
    "retweets = []\n",
    "not_parsed = []\n",
    "\n",
    "# iterate through tweets\n",
    "with open(year + '-all.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        tweet = json.loads(line)\n",
    "    \n",
    "        if line == None or tweet == None:\n",
    "            not_parsed.append((line, tweet))\n",
    "            print(\"Parsing error: \", line, tweet)\n",
    "        elif tweet['retweetedTweet']:\n",
    "            retweets.append(tweet)\n",
    "            print(\"Retweet: \", tweet['id'])\n",
    "        else: \n",
    "            # clean tweet content\n",
    "            tweet_text = tweet['rawContent'] # note: need other prop for over 140 char?\n",
    "            remove_emojis = demoji.replace(tweet_text, \"\")\n",
    "            remove_more_emojis = re.sub(\"([\\uD83E-\\uD83E])+\", \"\", remove_emojis)\n",
    "            remove_newlines = re.sub(\"(\\n)+\", \"\", remove_more_emojis)\n",
    "            remove_usernames = re.sub(\"@([a-zA-Z0-9_]+)\", \"\", remove_newlines)\n",
    "            remove_hashtags = re.sub(\"#([a-zA-Z0-9_ぁ-んァ-ン一-龠]+)\", \"\", remove_usernames)\n",
    "            remove_links = re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"\", remove_hashtags)\n",
    "            remove_punc = re.sub(\"([-.,;\\\"\\'!?~@#$%^&*():\\{\\}\\[\\]\\/\\\\\\\\]+)\", \"\", remove_links)\n",
    "            remove_jp_punc = re.sub(\"([\\uFF01-\\uFF0F\\uFF1A-\\uFF20\\uFF3B-\\uFF40\\uFF5B-\\uFF65\\uFF9E-\\uFFEE\\u3000-\\u303F]+)\", \"\", remove_punc)\n",
    "            remove_geo_shapes = re.sub(\"([\\u25A0-\\u25FF])+\", \"\", remove_jp_punc)\n",
    "            remove_misc_symbols = re.sub(\"([\\u2600-\\u26FF])+\", \"\", remove_geo_shapes)\n",
    "\n",
    "            # tokenize with mecab\n",
    "            parsed = mt.parseToNode(remove_misc_symbols)\n",
    "            components = []\n",
    "            while parsed:\n",
    "                components.append(parsed.surface)\n",
    "                parsed = parsed.next\n",
    "            components = [token for token in components if not token in stop_words]\n",
    "            tokens.append(components)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# did we get retweets, or did twscrape filter them out?\n",
    "print(len(retweets))\n",
    "\n",
    "# did any tweets fail parsing?\n",
    "print(len(not_parsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 11:56:05,171 : INFO : collecting all words and their counts\n",
      "2023-11-16 11:56:05,172 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-11-16 11:56:05,205 : INFO : PROGRESS: at sentence #10000, processed 112838 words, keeping 19834 word types\n",
      "2023-11-16 11:56:05,228 : INFO : PROGRESS: at sentence #20000, processed 222965 words, keeping 24995 word types\n",
      "2023-11-16 11:56:05,263 : INFO : PROGRESS: at sentence #30000, processed 338097 words, keeping 38689 word types\n",
      "2023-11-16 11:56:05,287 : INFO : PROGRESS: at sentence #40000, processed 448583 words, keeping 49200 word types\n",
      "2023-11-16 11:56:05,312 : INFO : PROGRESS: at sentence #50000, processed 558436 words, keeping 54062 word types\n",
      "2023-11-16 11:56:05,336 : INFO : PROGRESS: at sentence #60000, processed 672942 words, keeping 56539 word types\n",
      "2023-11-16 11:56:05,361 : INFO : PROGRESS: at sentence #70000, processed 786832 words, keeping 62302 word types\n",
      "2023-11-16 11:56:05,401 : INFO : PROGRESS: at sentence #80000, processed 896866 words, keeping 62954 word types\n",
      "2023-11-16 11:56:05,435 : INFO : PROGRESS: at sentence #90000, processed 1008243 words, keeping 67843 word types\n",
      "2023-11-16 11:56:05,465 : INFO : PROGRESS: at sentence #100000, processed 1119912 words, keeping 70233 word types\n",
      "2023-11-16 11:56:05,506 : INFO : PROGRESS: at sentence #110000, processed 1233016 words, keeping 77708 word types\n",
      "2023-11-16 11:56:05,535 : INFO : PROGRESS: at sentence #120000, processed 1347846 words, keeping 85058 word types\n",
      "2023-11-16 11:56:05,562 : INFO : PROGRESS: at sentence #130000, processed 1462714 words, keeping 88559 word types\n",
      "2023-11-16 11:56:05,599 : INFO : PROGRESS: at sentence #140000, processed 1573374 words, keeping 92606 word types\n",
      "2023-11-16 11:56:05,624 : INFO : PROGRESS: at sentence #150000, processed 1687235 words, keeping 93223 word types\n",
      "2023-11-16 11:56:05,654 : INFO : PROGRESS: at sentence #160000, processed 1802632 words, keeping 94166 word types\n",
      "2023-11-16 11:56:05,681 : INFO : PROGRESS: at sentence #170000, processed 1914825 words, keeping 99292 word types\n",
      "2023-11-16 11:56:05,718 : INFO : PROGRESS: at sentence #180000, processed 2031046 words, keeping 103643 word types\n",
      "2023-11-16 11:56:05,746 : INFO : PROGRESS: at sentence #190000, processed 2140922 words, keeping 108193 word types\n",
      "2023-11-16 11:56:05,770 : INFO : PROGRESS: at sentence #200000, processed 2247517 words, keeping 113148 word types\n",
      "2023-11-16 11:56:05,797 : INFO : PROGRESS: at sentence #210000, processed 2355761 words, keeping 117999 word types\n",
      "2023-11-16 11:56:05,822 : INFO : PROGRESS: at sentence #220000, processed 2463867 words, keeping 122883 word types\n",
      "2023-11-16 11:56:05,848 : INFO : PROGRESS: at sentence #230000, processed 2569174 words, keeping 127407 word types\n",
      "2023-11-16 11:56:05,886 : INFO : PROGRESS: at sentence #240000, processed 2677007 words, keeping 131436 word types\n",
      "2023-11-16 11:56:05,917 : INFO : PROGRESS: at sentence #250000, processed 2788542 words, keeping 135693 word types\n",
      "2023-11-16 11:56:05,943 : INFO : PROGRESS: at sentence #260000, processed 2895900 words, keeping 137916 word types\n",
      "2023-11-16 11:56:05,971 : INFO : PROGRESS: at sentence #270000, processed 3004793 words, keeping 141528 word types\n",
      "2023-11-16 11:56:06,001 : INFO : PROGRESS: at sentence #280000, processed 3113272 words, keeping 145267 word types\n",
      "2023-11-16 11:56:06,043 : INFO : PROGRESS: at sentence #290000, processed 3224638 words, keeping 149571 word types\n",
      "2023-11-16 11:56:06,075 : INFO : PROGRESS: at sentence #300000, processed 3337307 words, keeping 152652 word types\n",
      "2023-11-16 11:56:06,105 : INFO : PROGRESS: at sentence #310000, processed 3451606 words, keeping 155891 word types\n",
      "2023-11-16 11:56:06,133 : INFO : PROGRESS: at sentence #320000, processed 3562965 words, keeping 159943 word types\n",
      "2023-11-16 11:56:06,161 : INFO : PROGRESS: at sentence #330000, processed 3675408 words, keeping 162534 word types\n",
      "2023-11-16 11:56:06,201 : INFO : PROGRESS: at sentence #340000, processed 3787381 words, keeping 166211 word types\n",
      "2023-11-16 11:56:06,232 : INFO : PROGRESS: at sentence #350000, processed 3897530 words, keeping 167918 word types\n",
      "2023-11-16 11:56:06,257 : INFO : PROGRESS: at sentence #360000, processed 4006360 words, keeping 170366 word types\n",
      "2023-11-16 11:56:06,286 : INFO : PROGRESS: at sentence #370000, processed 4117642 words, keeping 172358 word types\n",
      "2023-11-16 11:56:06,330 : INFO : PROGRESS: at sentence #380000, processed 4227367 words, keeping 174644 word types\n",
      "2023-11-16 11:56:06,363 : INFO : PROGRESS: at sentence #390000, processed 4336621 words, keeping 178346 word types\n",
      "2023-11-16 11:56:06,410 : INFO : PROGRESS: at sentence #400000, processed 4445409 words, keeping 181929 word types\n",
      "2023-11-16 11:56:06,438 : INFO : PROGRESS: at sentence #410000, processed 4550220 words, keeping 185480 word types\n",
      "2023-11-16 11:56:06,462 : INFO : PROGRESS: at sentence #420000, processed 4656128 words, keeping 188910 word types\n",
      "2023-11-16 11:56:06,488 : INFO : PROGRESS: at sentence #430000, processed 4762378 words, keeping 191959 word types\n",
      "2023-11-16 11:56:06,522 : INFO : PROGRESS: at sentence #440000, processed 4868499 words, keeping 194245 word types\n",
      "2023-11-16 11:56:06,557 : INFO : PROGRESS: at sentence #450000, processed 4976254 words, keeping 196206 word types\n",
      "2023-11-16 11:56:06,586 : INFO : PROGRESS: at sentence #460000, processed 5081749 words, keeping 199582 word types\n",
      "2023-11-16 11:56:06,613 : INFO : PROGRESS: at sentence #470000, processed 5192155 words, keeping 202693 word types\n",
      "2023-11-16 11:56:06,643 : INFO : PROGRESS: at sentence #480000, processed 5313601 words, keeping 205331 word types\n",
      "2023-11-16 11:56:06,680 : INFO : PROGRESS: at sentence #490000, processed 5430900 words, keeping 206047 word types\n",
      "2023-11-16 11:56:06,717 : INFO : PROGRESS: at sentence #500000, processed 5541782 words, keeping 208994 word types\n",
      "2023-11-16 11:56:06,747 : INFO : PROGRESS: at sentence #510000, processed 5658879 words, keeping 211228 word types\n",
      "2023-11-16 11:56:06,773 : INFO : PROGRESS: at sentence #520000, processed 5767244 words, keeping 212414 word types\n",
      "2023-11-16 11:56:06,802 : INFO : PROGRESS: at sentence #530000, processed 5879506 words, keeping 212869 word types\n",
      "2023-11-16 11:56:06,830 : INFO : PROGRESS: at sentence #540000, processed 5990410 words, keeping 213895 word types\n",
      "2023-11-16 11:56:06,876 : INFO : PROGRESS: at sentence #550000, processed 6107883 words, keeping 214729 word types\n",
      "2023-11-16 11:56:06,907 : INFO : PROGRESS: at sentence #560000, processed 6226423 words, keeping 214872 word types\n",
      "2023-11-16 11:56:06,924 : INFO : collected 214897 word types from a corpus of 6291320 raw words and 565434 sentences\n",
      "2023-11-16 11:56:06,924 : INFO : Creating a fresh vocabulary\n",
      "2023-11-16 11:56:07,043 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 39200 unique words (18.24% of original 214897, drops 175697)', 'datetime': '2023-11-16T11:56:07.043260', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-16 11:56:07,044 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 5856922 word corpus (93.10% of original 6291320, drops 434398)', 'datetime': '2023-11-16T11:56:07.043989', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-16 11:56:07,182 : INFO : deleting the raw counts dictionary of 214897 items\n",
      "2023-11-16 11:56:07,187 : INFO : sample=0.001 downsamples 25 most-common words\n",
      "2023-11-16 11:56:07,188 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 5460983.3631500825 word corpus (93.2%% of prior 5856922)', 'datetime': '2023-11-16T11:56:07.188263', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-16 11:56:07,428 : INFO : estimated required memory for 39200 words and 100 dimensions: 50960000 bytes\n",
      "2023-11-16 11:56:07,429 : INFO : resetting layer weights\n",
      "2023-11-16 11:56:07,446 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-11-16T11:56:07.446274', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2023-11-16 11:56:07,447 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 39200 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-11-16T11:56:07.447182', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-11-16 11:56:08,454 : INFO : EPOCH 0 - PROGRESS: at 27.12% examples, 1495964 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:09,458 : INFO : EPOCH 0 - PROGRESS: at 57.27% examples, 1558048 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:10,465 : INFO : EPOCH 0 - PROGRESS: at 84.31% examples, 1516554 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:11,106 : INFO : EPOCH 0: training on 6291320 raw words (5460798 effective words) took 3.7s, 1494958 effective words/s\n",
      "2023-11-16 11:56:12,114 : INFO : EPOCH 1 - PROGRESS: at 30.72% examples, 1690595 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:13,115 : INFO : EPOCH 1 - PROGRESS: at 60.56% examples, 1648615 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:14,116 : INFO : EPOCH 1 - PROGRESS: at 88.29% examples, 1595547 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:14,595 : INFO : EPOCH 1: training on 6291320 raw words (5460179 effective words) took 3.5s, 1566864 effective words/s\n",
      "2023-11-16 11:56:15,600 : INFO : EPOCH 2 - PROGRESS: at 26.23% examples, 1444955 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:16,602 : INFO : EPOCH 2 - PROGRESS: at 56.31% examples, 1533866 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:17,604 : INFO : EPOCH 2 - PROGRESS: at 87.78% examples, 1587763 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:18,049 : INFO : EPOCH 2: training on 6291320 raw words (5461260 effective words) took 3.4s, 1583083 effective words/s\n",
      "2023-11-16 11:56:19,055 : INFO : EPOCH 3 - PROGRESS: at 31.31% examples, 1728283 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:20,055 : INFO : EPOCH 3 - PROGRESS: at 64.12% examples, 1746468 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:21,056 : INFO : EPOCH 3 - PROGRESS: at 97.47% examples, 1768974 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:21,138 : INFO : EPOCH 3: training on 6291320 raw words (5461524 effective words) took 3.1s, 1770608 effective words/s\n",
      "2023-11-16 11:56:22,149 : INFO : EPOCH 4 - PROGRESS: at 32.57% examples, 1786571 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:23,150 : INFO : EPOCH 4 - PROGRESS: at 64.59% examples, 1753831 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:24,156 : INFO : EPOCH 4 - PROGRESS: at 96.29% examples, 1738746 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-16 11:56:24,314 : INFO : EPOCH 4: training on 6291320 raw words (5460987 effective words) took 3.2s, 1721426 effective words/s\n",
      "2023-11-16 11:56:24,315 : INFO : Word2Vec lifecycle event {'msg': 'training on 31456600 raw words (27304748 effective words) took 16.9s, 1618806 effective words/s', 'datetime': '2023-11-16T11:56:24.314948', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-11-16 11:56:24,315 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=39200, vector_size=100, alpha=0.025>', 'datetime': '2023-11-16T11:56:24.315377', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# train word2vec model\n",
    "model = gensim.models.Word2Vec(tokens, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 11:56:24,323 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'thesis_w2v_2022_tweets', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-11-16T11:56:24.323415', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2023-11-16 11:56:24,334 : INFO : not storing attribute cum_table\n",
      "2023-11-16 11:56:24,421 : INFO : saved thesis_w2v_2022_tweets\n"
     ]
    }
   ],
   "source": [
    "# save word2vec model\n",
    "model.save(\"thesis_w2v_\" + year + \"_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 11:57:19,137 : INFO : loading Word2Vec object from thesis_w2v_2022_tweets\n",
      "2023-11-16 11:57:19,198 : INFO : loading wv recursively from thesis_w2v_2022_tweets.wv.* with mmap=None\n",
      "2023-11-16 11:57:19,199 : INFO : setting ignored attribute cum_table to None\n",
      "2023-11-16 11:57:19,433 : INFO : Word2Vec lifecycle event {'fname': 'thesis_w2v_2022_tweets', 'datetime': '2023-11-16T11:57:19.433173', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'loaded'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x110e53a90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained word2vec model\n",
    "gensim.models.Word2Vec.load(\"thesis_w2v_\" + year + \"_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('侵略者', 0.8887750506401062), ('軍隊', 0.8882155418395996), ('民意', 0.883852481842041), ('略奪', 0.8815134167671204), ('統一協会', 0.8762111663818359), ('全土', 0.8744518756866455), ('陰謀', 0.8732355237007141), ('ネオコン', 0.8732097148895264), ('食糧', 0.8714119791984558), ('裁判官', 0.8696999549865723)]\n",
      "[('中国人', 0.887516975402832), ('ユダヤ教', 0.870999276638031), ('イスラム教', 0.8588070869445801), ('ユダヤ人', 0.8398546576499939), ('留学生', 0.838367760181427), ('組織', 0.834230899810791), ('欧米', 0.8304198980331421), ('日本人', 0.8295359015464783), ('中国', 0.8283986449241638), ('支持', 0.827479898929596)]\n"
     ]
    }
   ],
   "source": [
    "# check similarity given by trained model\n",
    "print(model.wv.most_similar(positive='在日',topn=10))\n",
    "print(model.wv.most_similar(positive='外国人',topn=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
