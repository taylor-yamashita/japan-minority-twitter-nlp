{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "import json\n",
    "import MeCab\n",
    "import demoji\n",
    "import re\n",
    "from stop_words import stop_words\n",
    "\n",
    "# word2vec\n",
    "import gensim, logging\n",
    "\n",
    "# plotting\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.manifold import TSNE               \n",
    "import numpy as np                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and Tokenize Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize with mecab\n",
    "mt = MeCab.Tagger(\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "# dataset we are working with\n",
    "year = \"2015\"\n",
    "\n",
    "# store results and exception tweets\n",
    "tokens = []\n",
    "retweets = []\n",
    "not_parsed = []\n",
    "\n",
    "# iterate through tweets\n",
    "with open(year + '-all.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        tweet = json.loads(line)\n",
    "    \n",
    "        if line == None or tweet == None:\n",
    "            not_parsed.append((line, tweet))\n",
    "            print(\"Parsing error: \", line, tweet)\n",
    "        elif tweet['retweetedTweet']:\n",
    "            retweets.append(tweet)\n",
    "            print(\"Retweet: \", tweet['id'])\n",
    "        else: \n",
    "            # clean tweet content\n",
    "            tweet_text = tweet['rawContent'] # note: need other prop for over 140 char?\n",
    "            remove_emojis = demoji.replace(tweet_text, \"\")\n",
    "            remove_more_emojis = re.sub(\"([\\uD83E-\\uD83E])+\", \"\", remove_emojis)\n",
    "            remove_newlines = re.sub(\"(\\n)+\", \"\", remove_more_emojis)\n",
    "            remove_usernames = re.sub(\"@([a-zA-Z0-9_]+)\", \"\", remove_newlines)\n",
    "            remove_hashtags = re.sub(\"#([a-zA-Z0-9_ぁ-んァ-ン一-龠]+)\", \"\", remove_usernames)\n",
    "            remove_links = re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"\", remove_hashtags)\n",
    "            remove_punc = re.sub(\"([-.,;\\\"\\'!?~@#$%^&*():\\{\\}\\[\\]\\/\\\\\\\\]+)\", \"\", remove_links)\n",
    "            remove_jp_punc = re.sub(\"([\\uFF01-\\uFF0F\\uFF1A-\\uFF20\\uFF3B-\\uFF40\\uFF5B-\\uFF65\\uFF9E-\\uFFEE\\u3000-\\u303F]+)\", \"\", remove_punc)\n",
    "            remove_geo_shapes = re.sub(\"([\\u25A0-\\u25FF])+\", \"\", remove_jp_punc)\n",
    "            remove_misc_symbols = re.sub(\"([\\u2600-\\u26FF])+\", \"\", remove_geo_shapes)\n",
    "\n",
    "            # tokenize with mecab\n",
    "            parsed = mt.parseToNode(remove_misc_symbols)\n",
    "            components = []\n",
    "            while parsed:\n",
    "                components.append(parsed.surface)\n",
    "                parsed = parsed.next\n",
    "            components = [token for token in components if not token in stop_words]\n",
    "            tokens.append(components)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# did we get retweets, or did twscrape filter them out?\n",
    "print(len(retweets))\n",
    "\n",
    "# did any tweets fail parsing?\n",
    "print(len(not_parsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 11:45:01,537 : INFO : collecting all words and their counts\n",
      "2023-11-30 11:45:01,538 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-11-30 11:45:01,570 : INFO : PROGRESS: at sentence #10000, processed 114228 words, keeping 24650 word types\n",
      "2023-11-30 11:45:01,604 : INFO : PROGRESS: at sentence #20000, processed 231458 words, keeping 34938 word types\n",
      "2023-11-30 11:45:01,632 : INFO : PROGRESS: at sentence #30000, processed 339402 words, keeping 46351 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 11:45:01,658 : INFO : PROGRESS: at sentence #40000, processed 453140 words, keeping 55661 word types\n",
      "2023-11-30 11:45:01,700 : INFO : PROGRESS: at sentence #50000, processed 565107 words, keeping 63789 word types\n",
      "2023-11-30 11:45:01,733 : INFO : PROGRESS: at sentence #60000, processed 674237 words, keeping 66590 word types\n",
      "2023-11-30 11:45:01,756 : INFO : PROGRESS: at sentence #70000, processed 784038 words, keeping 67884 word types\n",
      "2023-11-30 11:45:01,779 : INFO : PROGRESS: at sentence #80000, processed 898438 words, keeping 68752 word types\n",
      "2023-11-30 11:45:01,806 : INFO : PROGRESS: at sentence #90000, processed 1033998 words, keeping 77940 word types\n",
      "2023-11-30 11:45:01,839 : INFO : PROGRESS: at sentence #100000, processed 1143976 words, keeping 84003 word types\n",
      "2023-11-30 11:45:01,864 : INFO : PROGRESS: at sentence #110000, processed 1253336 words, keeping 87153 word types\n",
      "2023-11-30 11:45:01,888 : INFO : PROGRESS: at sentence #120000, processed 1363467 words, keeping 92244 word types\n",
      "2023-11-30 11:45:01,911 : INFO : PROGRESS: at sentence #130000, processed 1472004 words, keeping 96904 word types\n",
      "2023-11-30 11:45:01,934 : INFO : PROGRESS: at sentence #140000, processed 1580311 words, keeping 99020 word types\n",
      "2023-11-30 11:45:01,956 : INFO : PROGRESS: at sentence #150000, processed 1685633 words, keeping 103781 word types\n",
      "2023-11-30 11:45:01,978 : INFO : PROGRESS: at sentence #160000, processed 1787179 words, keeping 108528 word types\n",
      "2023-11-30 11:45:02,007 : INFO : PROGRESS: at sentence #170000, processed 1892597 words, keeping 113515 word types\n",
      "2023-11-30 11:45:02,048 : INFO : PROGRESS: at sentence #180000, processed 2003789 words, keeping 118971 word types\n",
      "2023-11-30 11:45:02,071 : INFO : PROGRESS: at sentence #190000, processed 2117713 words, keeping 124191 word types\n",
      "2023-11-30 11:45:02,095 : INFO : PROGRESS: at sentence #200000, processed 2230830 words, keeping 128983 word types\n",
      "2023-11-30 11:45:02,119 : INFO : PROGRESS: at sentence #210000, processed 2347549 words, keeping 134076 word types\n",
      "2023-11-30 11:45:02,145 : INFO : PROGRESS: at sentence #220000, processed 2465595 words, keeping 138939 word types\n",
      "2023-11-30 11:45:02,170 : INFO : PROGRESS: at sentence #230000, processed 2580421 words, keeping 143734 word types\n",
      "2023-11-30 11:45:02,213 : INFO : PROGRESS: at sentence #240000, processed 2698266 words, keeping 148476 word types\n",
      "2023-11-30 11:45:02,238 : INFO : PROGRESS: at sentence #250000, processed 2812351 words, keeping 152816 word types\n",
      "2023-11-30 11:45:02,263 : INFO : PROGRESS: at sentence #260000, processed 2926706 words, keeping 157056 word types\n",
      "2023-11-30 11:45:02,288 : INFO : PROGRESS: at sentence #270000, processed 3041838 words, keeping 161288 word types\n",
      "2023-11-30 11:45:02,326 : INFO : PROGRESS: at sentence #280000, processed 3157428 words, keeping 165467 word types\n",
      "2023-11-30 11:45:02,352 : INFO : PROGRESS: at sentence #290000, processed 3274787 words, keeping 169642 word types\n",
      "2023-11-30 11:45:02,378 : INFO : PROGRESS: at sentence #300000, processed 3392668 words, keeping 173864 word types\n",
      "2023-11-30 11:45:02,427 : INFO : PROGRESS: at sentence #310000, processed 3511286 words, keeping 178033 word types\n",
      "2023-11-30 11:45:02,465 : INFO : PROGRESS: at sentence #320000, processed 3623715 words, keeping 181818 word types\n",
      "2023-11-30 11:45:02,491 : INFO : PROGRESS: at sentence #330000, processed 3739791 words, keeping 185617 word types\n",
      "2023-11-30 11:45:02,519 : INFO : PROGRESS: at sentence #340000, processed 3865326 words, keeping 191203 word types\n",
      "2023-11-30 11:45:02,542 : INFO : PROGRESS: at sentence #350000, processed 3969018 words, keeping 194803 word types\n",
      "2023-11-30 11:45:02,567 : INFO : PROGRESS: at sentence #360000, processed 4081377 words, keeping 198953 word types\n",
      "2023-11-30 11:45:02,605 : INFO : PROGRESS: at sentence #370000, processed 4187349 words, keeping 202509 word types\n",
      "2023-11-30 11:45:02,630 : INFO : PROGRESS: at sentence #380000, processed 4297648 words, keeping 206256 word types\n",
      "2023-11-30 11:45:02,655 : INFO : PROGRESS: at sentence #390000, processed 4412210 words, keeping 209806 word types\n",
      "2023-11-30 11:45:02,684 : INFO : PROGRESS: at sentence #400000, processed 4526624 words, keeping 213132 word types\n",
      "2023-11-30 11:45:02,729 : INFO : PROGRESS: at sentence #410000, processed 4638111 words, keeping 216413 word types\n",
      "2023-11-30 11:45:02,756 : INFO : PROGRESS: at sentence #420000, processed 4754059 words, keeping 219728 word types\n",
      "2023-11-30 11:45:02,781 : INFO : PROGRESS: at sentence #430000, processed 4867839 words, keeping 223059 word types\n",
      "2023-11-30 11:45:02,813 : INFO : PROGRESS: at sentence #440000, processed 4981802 words, keeping 226474 word types\n",
      "2023-11-30 11:45:02,842 : INFO : PROGRESS: at sentence #450000, processed 5098859 words, keeping 230075 word types\n",
      "2023-11-30 11:45:02,868 : INFO : PROGRESS: at sentence #460000, processed 5214166 words, keeping 233409 word types\n",
      "2023-11-30 11:45:02,893 : INFO : PROGRESS: at sentence #470000, processed 5328518 words, keeping 236650 word types\n",
      "2023-11-30 11:45:02,920 : INFO : PROGRESS: at sentence #480000, processed 5446885 words, keeping 240534 word types\n",
      "2023-11-30 11:45:02,952 : INFO : PROGRESS: at sentence #490000, processed 5553730 words, keeping 243701 word types\n",
      "2023-11-30 11:45:02,977 : INFO : PROGRESS: at sentence #500000, processed 5663953 words, keeping 247065 word types\n",
      "2023-11-30 11:45:03,004 : INFO : PROGRESS: at sentence #510000, processed 5779761 words, keeping 250224 word types\n",
      "2023-11-30 11:45:03,028 : INFO : PROGRESS: at sentence #520000, processed 5885562 words, keeping 252910 word types\n",
      "2023-11-30 11:45:03,072 : INFO : PROGRESS: at sentence #530000, processed 5999543 words, keeping 255932 word types\n",
      "2023-11-30 11:45:03,096 : INFO : PROGRESS: at sentence #540000, processed 6108448 words, keeping 258814 word types\n",
      "2023-11-30 11:45:03,122 : INFO : PROGRESS: at sentence #550000, processed 6221630 words, keeping 261885 word types\n",
      "2023-11-30 11:45:03,145 : INFO : PROGRESS: at sentence #560000, processed 6336568 words, keeping 264937 word types\n",
      "2023-11-30 11:45:03,170 : INFO : PROGRESS: at sentence #570000, processed 6452300 words, keeping 267994 word types\n",
      "2023-11-30 11:45:03,174 : INFO : collected 268372 word types from a corpus of 6467726 raw words and 571403 sentences\n",
      "2023-11-30 11:45:03,175 : INFO : Creating a fresh vocabulary\n",
      "2023-11-30 11:45:03,302 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 39579 unique words (14.75% of original 268372, drops 228793)', 'datetime': '2023-11-30T11:45:03.302106', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-30 11:45:03,302 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 5991021 word corpus (92.63% of original 6467726, drops 476705)', 'datetime': '2023-11-30T11:45:03.302656', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-30 11:45:03,437 : INFO : deleting the raw counts dictionary of 268372 items\n",
      "2023-11-30 11:45:03,442 : INFO : sample=0.001 downsamples 27 most-common words\n",
      "2023-11-30 11:45:03,443 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 5564749.878847666 word corpus (92.9%% of prior 5991021)', 'datetime': '2023-11-30T11:45:03.443658', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-30 11:45:03,693 : INFO : estimated required memory for 39579 words and 100 dimensions: 51452700 bytes\n",
      "2023-11-30 11:45:03,693 : INFO : resetting layer weights\n",
      "2023-11-30 11:45:03,716 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-11-30T11:45:03.716701', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2023-11-30 11:45:03,717 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 39579 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-11-30T11:45:03.717250', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-11-30 11:45:04,734 : INFO : EPOCH 0 - PROGRESS: at 33.75% examples, 1841449 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-30 11:45:05,735 : INFO : EPOCH 0 - PROGRESS: at 66.16% examples, 1835642 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-30 11:45:06,736 : INFO : EPOCH 0 - PROGRESS: at 99.16% examples, 1835970 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-30 11:45:06,759 : INFO : EPOCH 0: training on 6467726 raw words (5564406 effective words) took 3.0s, 1837720 effective words/s\n",
      "2023-11-30 11:45:07,768 : INFO : EPOCH 1 - PROGRESS: at 32.17% examples, 1754548 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-30 11:45:08,768 : INFO : EPOCH 1 - PROGRESS: at 62.59% examples, 1741409 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-30 11:45:09,770 : INFO : EPOCH 1 - PROGRESS: at 92.22% examples, 1707990 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-30 11:45:10,003 : INFO : EPOCH 1: training on 6467726 raw words (5564762 effective words) took 3.2s, 1718123 effective words/s\n",
      "2023-11-30 11:45:11,012 : INFO : EPOCH 2 - PROGRESS: at 33.59% examples, 1828959 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-30 11:45:12,015 : INFO : EPOCH 2 - PROGRESS: at 66.78% examples, 1848987 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-30 11:45:13,001 : INFO : EPOCH 2: training on 6467726 raw words (5564514 effective words) took 3.0s, 1858054 effective words/s\n",
      "2023-11-30 11:45:14,008 : INFO : EPOCH 3 - PROGRESS: at 33.90% examples, 1850064 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-30 11:45:15,009 : INFO : EPOCH 3 - PROGRESS: at 66.00% examples, 1831190 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-30 11:45:16,012 : INFO : EPOCH 3 - PROGRESS: at 99.70% examples, 1845147 words/s, in_qsize 2, out_qsize 1\n",
      "2023-11-30 11:45:16,016 : INFO : EPOCH 3: training on 6467726 raw words (5564512 effective words) took 3.0s, 1848310 effective words/s\n",
      "2023-11-30 11:45:17,020 : INFO : EPOCH 4 - PROGRESS: at 33.90% examples, 1854017 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-30 11:45:18,020 : INFO : EPOCH 4 - PROGRESS: at 66.78% examples, 1855687 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-30 11:45:19,020 : INFO : EPOCH 4 - PROGRESS: at 100.00% examples, 1854341 words/s, in_qsize 0, out_qsize 1\n",
      "2023-11-30 11:45:19,020 : INFO : EPOCH 4: training on 6467726 raw words (5564474 effective words) took 3.0s, 1854063 effective words/s\n",
      "2023-11-30 11:45:19,021 : INFO : Word2Vec lifecycle event {'msg': 'training on 32338630 raw words (27822668 effective words) took 15.3s, 1818049 effective words/s', 'datetime': '2023-11-30T11:45:19.021191', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-11-30 11:45:19,021 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=39579, vector_size=100, alpha=0.025>', 'datetime': '2023-11-30T11:45:19.021538', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# train word2vec model\n",
    "model = gensim.models.Word2Vec(tokens, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 11:45:19,091 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'thesis_w2v_2015_tweets', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-11-30T11:45:19.091673', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2023-11-30 11:45:19,098 : INFO : not storing attribute cum_table\n",
      "2023-11-30 11:45:19,182 : INFO : saved thesis_w2v_2015_tweets\n"
     ]
    }
   ],
   "source": [
    "# save word2vec model\n",
    "model.save(\"thesis_w2v_\" + year + \"_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 11:55:40,027 : INFO : loading Word2Vec object from thesis_w2v_2015_tweets\n",
      "2023-11-30 11:55:40,106 : INFO : loading wv recursively from thesis_w2v_2015_tweets.wv.* with mmap=None\n",
      "2023-11-30 11:55:40,107 : INFO : setting ignored attribute cum_table to None\n",
      "2023-11-30 11:55:40,346 : INFO : Word2Vec lifecycle event {'fname': 'thesis_w2v_2015_tweets', 'datetime': '2023-11-30T11:55:40.346216', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "# load trained word2vec model\n",
    "model_2015 = gensim.models.Word2Vec.load(\"thesis_w2v_\" + year + \"_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('侵攻', 0.8870046734809875), ('破る', 0.8761901259422302), ('人民', 0.8755786418914795), ('岸田文雄', 0.8644543886184692), ('自公', 0.8606095910072327), ('右派', 0.8567697405815125), ('起訴', 0.854092538356781), ('立民', 0.8535529971122742), ('プロパガンダ', 0.8532180786132812), ('朝鮮人', 0.8528974056243896)]\n",
      "[('日本人', 0.8911892771720886), ('中国人', 0.8543750643730164), ('若者', 0.8528403639793396), ('他国', 0.8411896824836731), ('政治', 0.8321614861488342), ('国', 0.8315311670303345), ('高齢者', 0.8311120867729187), ('移民', 0.8291254639625549), ('国会議員', 0.828331708908081), ('職員', 0.8269411325454712)]\n"
     ]
    }
   ],
   "source": [
    "# check similarity given by trained model\n",
    "print(model_2015.wv.most_similar(positive='在日',topn=10))\n",
    "print(model_2015.wv.most_similar(positive='外国人',topn=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Word2Vec Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/taylory/Desktop/*Thesis/thesis/thesis.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/taylory/Desktop/%2AThesis/thesis/thesis.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# def display_pca_scatterplot(model, words):\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/taylory/Desktop/%2AThesis/thesis/thesis.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#     word_vectors = np.array([model[w] for w in words])\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/taylory/Desktop/%2AThesis/thesis/thesis.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/taylory/Desktop/%2AThesis/thesis/thesis.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/taylory/Desktop/%2AThesis/thesis/thesis.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# fit a 2d PCA model to the vectors\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/taylory/Desktop/%2AThesis/thesis/thesis.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m X \u001b[39m=\u001b[39m model_2015[model_2015\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mvocab]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/taylory/Desktop/%2AThesis/thesis/thesis.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/taylory/Desktop/%2AThesis/thesis/thesis.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m result \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39mfit_transform(X)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/thesis-hw276cHk/lib/python3.11/site-packages/gensim/models/keyedvectors.py:734\u001b[0m, in \u001b[0;36mKeyedVectors.vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    733\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvocab\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 734\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    735\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe vocab attribute was removed from KeyedVector in Gensim 4.0.0.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse KeyedVector\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms .key_to_index dict, .index_to_key list, and methods \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    739\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
