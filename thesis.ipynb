{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import json\n",
    "import MeCab\n",
    "import demoji\n",
    "import re\n",
    "from stop_words import stop_words\n",
    "import gensim, logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and Tokenize Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize with mecab\n",
    "mt = MeCab.Tagger(\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "# store results and exception tweets\n",
    "tokens = []\n",
    "retweets = []\n",
    "not_parsed = []\n",
    "\n",
    "# iterate through tweets\n",
    "with open('2022-all.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        tweet = json.loads(line)\n",
    "    \n",
    "        if line == None or tweet == None:\n",
    "            not_parsed.append((line, tweet))\n",
    "            print(\"Parsing error: \", line, tweet)\n",
    "        elif tweet['retweetedTweet']:\n",
    "            retweets.append(tweet)\n",
    "            print(\"Retweet: \", tweet['id'])\n",
    "        else: \n",
    "            # clean tweet content\n",
    "            tweet_text = tweet['rawContent'] # note: need other prop for over 140 char?\n",
    "            remove_emojis = demoji.replace(tweet_text, \"\")\n",
    "            remove_more_emojis = re.sub(\"([\\uD83E-\\uD83E])+\", \"\", remove_emojis)\n",
    "            remove_newlines = re.sub(\"(\\n)+\", \"\", remove_more_emojis)\n",
    "            remove_usernames = re.sub(\"@([a-zA-Z0-9_]+)\", \"\", remove_newlines)\n",
    "            remove_hashtags = re.sub(\"#([a-zA-Z0-9_ぁ-んァ-ン一-龠]+)\", \"\", remove_usernames)\n",
    "            remove_links = re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"\", remove_hashtags)\n",
    "            remove_punc = re.sub(\"([-.,;\\\"\\'!?~@#$%^&*():\\{\\}\\[\\]\\/\\\\\\\\]+)\", \"\", remove_links)\n",
    "            remove_jp_punc = re.sub(\"([\\uFF01-\\uFF0F\\uFF1A-\\uFF20\\uFF3B-\\uFF40\\uFF5B-\\uFF65\\uFF9E-\\uFFEE\\u3000-\\u303F]+)\", \"\", remove_punc)\n",
    "            remove_geo_shapes = re.sub(\"([\\u25A0-\\u25FF])+\", \"\", remove_jp_punc)\n",
    "            remove_misc_symbols = re.sub(\"([\\u2600-\\u26FF])+\", \"\", remove_geo_shapes)\n",
    "\n",
    "            # tokenize with mecab\n",
    "            parsed = mt.parseToNode(remove_misc_symbols)\n",
    "            components = []\n",
    "            while parsed:\n",
    "                components.append(parsed.surface)\n",
    "                parsed = parsed.next\n",
    "            components = [token for token in components if not token in stop_words]\n",
    "            tokens.append(components)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 23:24:31,753 : INFO : collecting all words and their counts\n",
      "2023-11-06 23:24:31,758 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-11-06 23:24:31,789 : INFO : PROGRESS: at sentence #10000, processed 112838 words, keeping 19834 word types\n",
      "2023-11-06 23:24:31,815 : INFO : PROGRESS: at sentence #20000, processed 222965 words, keeping 24995 word types\n",
      "2023-11-06 23:24:31,843 : INFO : PROGRESS: at sentence #30000, processed 338097 words, keeping 38689 word types\n",
      "2023-11-06 23:24:31,901 : INFO : PROGRESS: at sentence #40000, processed 448583 words, keeping 49200 word types\n",
      "2023-11-06 23:24:31,927 : INFO : PROGRESS: at sentence #50000, processed 558436 words, keeping 54062 word types\n",
      "2023-11-06 23:24:31,953 : INFO : PROGRESS: at sentence #60000, processed 672942 words, keeping 56539 word types\n",
      "2023-11-06 23:24:31,982 : INFO : PROGRESS: at sentence #70000, processed 786832 words, keeping 62302 word types\n",
      "2023-11-06 23:24:32,018 : INFO : PROGRESS: at sentence #80000, processed 896866 words, keeping 62954 word types\n",
      "2023-11-06 23:24:32,047 : INFO : PROGRESS: at sentence #90000, processed 1008243 words, keeping 67843 word types\n",
      "2023-11-06 23:24:32,075 : INFO : PROGRESS: at sentence #100000, processed 1119912 words, keeping 70233 word types\n",
      "2023-11-06 23:24:32,109 : INFO : PROGRESS: at sentence #110000, processed 1233016 words, keeping 77708 word types\n",
      "2023-11-06 23:24:32,142 : INFO : PROGRESS: at sentence #120000, processed 1347846 words, keeping 85058 word types\n",
      "2023-11-06 23:24:32,172 : INFO : PROGRESS: at sentence #130000, processed 1462714 words, keeping 88559 word types\n",
      "2023-11-06 23:24:32,199 : INFO : PROGRESS: at sentence #140000, processed 1573374 words, keeping 92606 word types\n",
      "2023-11-06 23:24:32,226 : INFO : PROGRESS: at sentence #150000, processed 1687235 words, keeping 93223 word types\n",
      "2023-11-06 23:24:32,258 : INFO : PROGRESS: at sentence #160000, processed 1802632 words, keeping 94166 word types\n",
      "2023-11-06 23:24:32,317 : INFO : PROGRESS: at sentence #170000, processed 1914825 words, keeping 99292 word types\n",
      "2023-11-06 23:24:32,346 : INFO : PROGRESS: at sentence #180000, processed 2031046 words, keeping 103643 word types\n",
      "2023-11-06 23:24:32,373 : INFO : PROGRESS: at sentence #190000, processed 2140922 words, keeping 108193 word types\n",
      "2023-11-06 23:24:32,400 : INFO : PROGRESS: at sentence #200000, processed 2247517 words, keeping 113148 word types\n",
      "2023-11-06 23:24:32,427 : INFO : PROGRESS: at sentence #210000, processed 2355761 words, keeping 117999 word types\n",
      "2023-11-06 23:24:32,467 : INFO : PROGRESS: at sentence #220000, processed 2463867 words, keeping 122883 word types\n",
      "2023-11-06 23:24:32,497 : INFO : PROGRESS: at sentence #230000, processed 2569174 words, keeping 127407 word types\n",
      "2023-11-06 23:24:32,524 : INFO : PROGRESS: at sentence #240000, processed 2677007 words, keeping 131436 word types\n",
      "2023-11-06 23:24:32,560 : INFO : PROGRESS: at sentence #250000, processed 2788542 words, keeping 135693 word types\n",
      "2023-11-06 23:24:32,601 : INFO : PROGRESS: at sentence #260000, processed 2895900 words, keeping 137916 word types\n",
      "2023-11-06 23:24:32,630 : INFO : PROGRESS: at sentence #270000, processed 3004793 words, keeping 141528 word types\n",
      "2023-11-06 23:24:32,658 : INFO : PROGRESS: at sentence #280000, processed 3113272 words, keeping 145267 word types\n",
      "2023-11-06 23:24:32,689 : INFO : PROGRESS: at sentence #290000, processed 3224638 words, keeping 149571 word types\n",
      "2023-11-06 23:24:32,737 : INFO : PROGRESS: at sentence #300000, processed 3337307 words, keeping 152652 word types\n",
      "2023-11-06 23:24:32,767 : INFO : PROGRESS: at sentence #310000, processed 3451606 words, keeping 155891 word types\n",
      "2023-11-06 23:24:32,796 : INFO : PROGRESS: at sentence #320000, processed 3562965 words, keeping 159943 word types\n",
      "2023-11-06 23:24:32,825 : INFO : PROGRESS: at sentence #330000, processed 3675408 words, keeping 162534 word types\n",
      "2023-11-06 23:24:32,877 : INFO : PROGRESS: at sentence #340000, processed 3787381 words, keeping 166211 word types\n",
      "2023-11-06 23:24:32,908 : INFO : PROGRESS: at sentence #350000, processed 3897530 words, keeping 167918 word types\n",
      "2023-11-06 23:24:32,935 : INFO : PROGRESS: at sentence #360000, processed 4006360 words, keeping 170366 word types\n",
      "2023-11-06 23:24:32,964 : INFO : PROGRESS: at sentence #370000, processed 4117642 words, keeping 172358 word types\n",
      "2023-11-06 23:24:32,990 : INFO : PROGRESS: at sentence #380000, processed 4227367 words, keeping 174644 word types\n",
      "2023-11-06 23:24:33,070 : INFO : PROGRESS: at sentence #390000, processed 4336621 words, keeping 178346 word types\n",
      "2023-11-06 23:24:33,098 : INFO : PROGRESS: at sentence #400000, processed 4445409 words, keeping 181929 word types\n",
      "2023-11-06 23:24:33,145 : INFO : PROGRESS: at sentence #410000, processed 4550220 words, keeping 185480 word types\n",
      "2023-11-06 23:24:33,171 : INFO : PROGRESS: at sentence #420000, processed 4656128 words, keeping 188910 word types\n",
      "2023-11-06 23:24:33,197 : INFO : PROGRESS: at sentence #430000, processed 4762378 words, keeping 191959 word types\n",
      "2023-11-06 23:24:33,224 : INFO : PROGRESS: at sentence #440000, processed 4868499 words, keeping 194245 word types\n",
      "2023-11-06 23:24:33,267 : INFO : PROGRESS: at sentence #450000, processed 4976254 words, keeping 196206 word types\n",
      "2023-11-06 23:24:33,303 : INFO : PROGRESS: at sentence #460000, processed 5081749 words, keeping 199582 word types\n",
      "2023-11-06 23:24:33,339 : INFO : PROGRESS: at sentence #470000, processed 5192155 words, keeping 202693 word types\n",
      "2023-11-06 23:24:33,378 : INFO : PROGRESS: at sentence #480000, processed 5313601 words, keeping 205331 word types\n",
      "2023-11-06 23:24:33,435 : INFO : PROGRESS: at sentence #490000, processed 5430900 words, keeping 206047 word types\n",
      "2023-11-06 23:24:33,470 : INFO : PROGRESS: at sentence #500000, processed 5541782 words, keeping 208994 word types\n",
      "2023-11-06 23:24:33,528 : INFO : PROGRESS: at sentence #510000, processed 5658879 words, keeping 211228 word types\n",
      "2023-11-06 23:24:33,559 : INFO : PROGRESS: at sentence #520000, processed 5767244 words, keeping 212414 word types\n",
      "2023-11-06 23:24:33,598 : INFO : PROGRESS: at sentence #530000, processed 5879506 words, keeping 212869 word types\n",
      "2023-11-06 23:24:33,628 : INFO : PROGRESS: at sentence #540000, processed 5990410 words, keeping 213895 word types\n",
      "2023-11-06 23:24:33,660 : INFO : PROGRESS: at sentence #550000, processed 6107883 words, keeping 214729 word types\n",
      "2023-11-06 23:24:33,700 : INFO : PROGRESS: at sentence #560000, processed 6226423 words, keeping 214872 word types\n",
      "2023-11-06 23:24:33,726 : INFO : collected 214897 word types from a corpus of 6291320 raw words and 565434 sentences\n",
      "2023-11-06 23:24:33,727 : INFO : Creating a fresh vocabulary\n",
      "2023-11-06 23:24:33,860 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 39200 unique words (18.24% of original 214897, drops 175697)', 'datetime': '2023-11-06T23:24:33.860840', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-06 23:24:33,861 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 5856922 word corpus (93.10% of original 6291320, drops 434398)', 'datetime': '2023-11-06T23:24:33.861599', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-06 23:24:34,000 : INFO : deleting the raw counts dictionary of 214897 items\n",
      "2023-11-06 23:24:34,004 : INFO : sample=0.001 downsamples 25 most-common words\n",
      "2023-11-06 23:24:34,005 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 5460983.3631500825 word corpus (93.2%% of prior 5856922)', 'datetime': '2023-11-06T23:24:34.005544', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-06 23:24:34,327 : INFO : estimated required memory for 39200 words and 100 dimensions: 50960000 bytes\n",
      "2023-11-06 23:24:34,328 : INFO : resetting layer weights\n",
      "2023-11-06 23:24:34,385 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-11-06T23:24:34.384991', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2023-11-06 23:24:34,386 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 39200 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-11-06T23:24:34.386186', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-11-06 23:24:35,414 : INFO : EPOCH 0 - PROGRESS: at 16.71% examples, 910762 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:36,414 : INFO : EPOCH 0 - PROGRESS: at 45.02% examples, 1225493 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:37,417 : INFO : EPOCH 0 - PROGRESS: at 73.93% examples, 1334878 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:38,373 : INFO : EPOCH 0: training on 6291320 raw words (5461609 effective words) took 4.0s, 1377613 effective words/s\n",
      "2023-11-06 23:24:39,391 : INFO : EPOCH 1 - PROGRESS: at 30.42% examples, 1674844 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:40,394 : INFO : EPOCH 1 - PROGRESS: at 60.72% examples, 1652653 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:41,400 : INFO : EPOCH 1 - PROGRESS: at 91.75% examples, 1656309 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:41,663 : INFO : EPOCH 1: training on 6291320 raw words (5460683 effective words) took 3.3s, 1667337 effective words/s\n",
      "2023-11-06 23:24:42,670 : INFO : EPOCH 2 - PROGRESS: at 31.63% examples, 1747156 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:43,672 : INFO : EPOCH 2 - PROGRESS: at 64.92% examples, 1766970 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:44,678 : INFO : EPOCH 2 - PROGRESS: at 93.92% examples, 1698643 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:44,913 : INFO : EPOCH 2: training on 6291320 raw words (5460659 effective words) took 3.2s, 1683645 effective words/s\n",
      "2023-11-06 23:24:45,924 : INFO : EPOCH 3 - PROGRESS: at 29.78% examples, 1636520 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:46,932 : INFO : EPOCH 3 - PROGRESS: at 58.99% examples, 1598810 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:47,934 : INFO : EPOCH 3 - PROGRESS: at 89.35% examples, 1611189 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:48,328 : INFO : EPOCH 3: training on 6291320 raw words (5460974 effective words) took 3.4s, 1601714 effective words/s\n",
      "2023-11-06 23:24:49,336 : INFO : EPOCH 4 - PROGRESS: at 30.11% examples, 1659226 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:50,338 : INFO : EPOCH 4 - PROGRESS: at 60.89% examples, 1657597 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:51,343 : INFO : EPOCH 4 - PROGRESS: at 93.15% examples, 1683708 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-06 23:24:51,574 : INFO : EPOCH 4: training on 6291320 raw words (5460492 effective words) took 3.2s, 1685509 effective words/s\n",
      "2023-11-06 23:24:51,574 : INFO : Word2Vec lifecycle event {'msg': 'training on 31456600 raw words (27304417 effective words) took 17.2s, 1588593 effective words/s', 'datetime': '2023-11-06T23:24:51.574628', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'train'}\n",
      "2023-11-06 23:24:51,575 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=39200, vector_size=100, alpha=0.025>', 'datetime': '2023-11-06T23:24:51.575026', 'gensim': '4.3.2', 'python': '3.11.5 (main, Aug 24 2023, 15:18:16) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.5-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# word2vec\n",
    "\n",
    "# set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# train word2vec\n",
    "model = gensim.models.Word2Vec(tokens, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('今週', 0.7725615501403809), ('本日', 0.6239190697669983), ('今日も一日', 0.6208266019821167), ('日曜', 0.5616788268089294), ('昨日', 0.5532968640327454), ('三連休', 0.5496022701263428), ('3連休', 0.5415098667144775), ('土曜日', 0.5411140322685242), ('日曜日', 0.5399312376976013), ('今夜', 0.5187978148460388)]\n"
     ]
    }
   ],
   "source": [
    "# check similarity given by trained model\n",
    "sim = model.wv.most_similar('今日')\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
