{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List\n",
    "import twscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Scraping general tweet set for given year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_directories(keyword_eng: str, year: str):\n",
    "    dir_path = year if keyword_eng == \"\" else keyword_eng + \"_\" + year\n",
    "    os.mkdir(dir_path)\n",
    "\n",
    "    # single digit month\n",
    "    for i in range(1,10):\n",
    "        path = dir_path + \"/\" + \"0\" + str(i)\n",
    "        os.mkdir(path) \n",
    "\n",
    "    # double digit month\n",
    "    for j in range(10,13):\n",
    "        path = dir_path + \"/\" + str(j)\n",
    "        os.mkdir(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape randomly sampled tweets for 10 days in given month\n",
    "def scrape_month_sampled_tweets(year: str, months: List[str], days_in_month: int):\n",
    "    range_days = list(range(1,days_in_month+1))\n",
    "    range_times = list(range(0,24))\n",
    "\n",
    "    # sample 10 random days and times of day for each month\n",
    "    for m in months: \n",
    "        month_path = year + \"/\" + m\n",
    "        days = sorted(random.sample(range_days, k=10))   # random days of month (no replacement)\n",
    "        times = random.choices(range_times, k=10)   # random hours of day (replacement)\n",
    "        \n",
    "        # scrape tweets for the 10 picked days and times\n",
    "        for t in range(10):     \n",
    "            day = \"0\" + str(days[t]) if days[t] < 10 else str(days[t])\n",
    "            time = \"0\" + str(times[t]) if times[t] < 10 else str(times[t])\n",
    "            date_string = year + '-' + m + '-' + day\n",
    "            day_path = month_path + \"/\" + year + \"-\" + m + \"-\" + day + \".txt\"\n",
    "            command = 'twscrape search \"since:' + date_string + '_' + time + ':00:00_UTC until:' + date_string + '_' + time + ':59:59_UTC lang:ja\"　> ' + day_path + ' --limit=4500'\n",
    "            os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape random sample of tweets (general content or keyword-search) from given year\n",
    "def scrape_year_sampled_tweets(year: str):\n",
    "    # 28 day months\n",
    "    scrape_month_sampled_tweets(year,[\"02\"], 28)    # omit leap year 29th days for simplicity...?\n",
    "\n",
    "    # 30 day months\n",
    "    months_30 = [\"04\",\"06\",\"09\",\"11\"]\n",
    "    scrape_month_sampled_tweets(year,months_30, 30)\n",
    "\n",
    "    # 31 day months\n",
    "    months_31 = [\"01\",\"03\",\"05\",\"07\",\"08\",\"10\",\"12\"]\n",
    "    scrape_month_sampled_tweets(year,months_31, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_general_txt_files(year: str, keyword_eng=\"\"):\n",
    "  # concatenate .txt files into one file per month\n",
    "  for root, dirs, files in os.walk(\"./\" + year):\n",
    "      for name in dirs:\n",
    "        month_path = os.path.join(root, name)\n",
    "        os.system(\"cat \" + month_path + \"/*.txt > \" + year + \"/\" + name + \".txt\")\n",
    "  \n",
    "  # concatenate month .txt files into one file for the year\n",
    "  os.system(\"cat \" + year + \"/*.txt > \" + year + \"-all.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping general year sample datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last took 1790m\n",
    "scrape_year_sampled_tweets(\"2015\")\n",
    "concatenate_general_txt_files(\"2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_year_sampled_tweets(\"2022\")\n",
    "concatenate_general_txt_files(\"2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Scraping minority keyword-related tweet set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape tweets containing keyword from every month of given year\n",
    "def scrape_keyword_month_tweets(year: str, months: List[str], days_in_month: int, keyword_jp: str, keyword_eng: str):\n",
    "    for m in months: \n",
    "        command = 'twscrape search \"' + keyword_jp + ' since:' + year + '-' + m + '-01_00:00:00_UTC until:' + year + '-' + m + '-' + str(days_in_month) + '_23:59:59_UTC lang:ja\"　> ' + keyword_eng + '_' + year + '/' + m + '.txt --limit=30000'\n",
    "        os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape all tweets containing keyword in given year\n",
    "def scrape_keyword_sampled_tweets(year: str, keyword_jp: str, keyword_eng: str):\n",
    "    os.mkdir(keyword_eng + \"_\" + year) \n",
    "\n",
    "    # 28 day months\n",
    "    scrape_keyword_month_tweets(year,[\"02\"], 28, keyword_jp, keyword_eng)    # omit leap year 29th days for simplicity...?\n",
    "\n",
    "    # 30 day months\n",
    "    months_30 = [\"04\",\"06\",\"09\",\"11\"]\n",
    "    scrape_keyword_month_tweets(year,months_30, 30, keyword_jp, keyword_eng)\n",
    "\n",
    "    # 31 day months\n",
    "    months_31 = [\"01\",\"03\",\"05\",\"07\",\"08\",\"10\",\"12\"]\n",
    "    scrape_keyword_month_tweets(year,months_31, 31, keyword_jp, keyword_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_keyword_txt_files(year: str, keyword_eng=\"\"):\n",
    "  # concatenate month .txt files into one file for the year\n",
    "  os.system(\"cat \" + keyword_eng + \"_\" + year + \"/\" + \"*.txt > \" + keyword_eng + \"_\" + year + \".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minority keyword-related datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zainichi korean\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"在日コリアン\", \"zainichi\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"zainichi\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"在日コリアン\", \"zainichi\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"zainichi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ainu\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"アイヌ\", \"ainu\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"ainu\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"アイヌ\", \"ainu\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"ainu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ryukyujin \n",
    "scrape_keyword_sampled_tweets(\"2022\", \"琉球人\", \"ryukyujin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"ryukyujin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"琉球人\", \"ryukyujin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"ryukyujin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not finished but didn't use\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"琉球\", \"ryukyu\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"ryukyu\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"琉球\", \"ryukyu\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"ryukyu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okinawajin\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"沖縄人\", \"okinawajin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"okinawajin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"沖縄人\", \"okinawajin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"okinawajin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# haafu\n",
    "# this one took around 100 min per month...\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"ハーフ\", \"haafu\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"haafu\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"ハーフ\", \"haafu\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"haafu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vietnam\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"ベトナム人\", \"vietnamjin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"vietnamjin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"ベトナム人\", \"vietnamjin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"vietnamjin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# philippines\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"フィリピン人\", \"philippinejin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"philippinejin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"フィリピン人\", \"philippinejin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"philippinejin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nepal\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"ネパール人\", \"nepaljin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"nepaljin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"ネパール人\", \"nepaljin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"nepaljin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indonesia\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"インドネシア人\", \"indonesiajin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"indonesiajin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"インドネシア人\", \"indonesiajin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"indonesiajin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zainichi kankokujin\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"在日韓国人\", \"zainichi_kankokujin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"zainichi_kankokujin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"在日韓国人\", \"zainichi_kankokujin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"zainichi_kankokujin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaijin [didn't do 2022; hold]\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"外人\", \"gaijin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"gaijin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"外人\", \"gaijin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"gaijin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-hw276cHk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
