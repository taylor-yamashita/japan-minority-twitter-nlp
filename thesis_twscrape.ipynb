{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twscrape\n",
      "  Downloading twscrape-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting aiosqlite>=0.17.0 (from twscrape)\n",
      "  Downloading aiosqlite-0.20.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting fake-useragent>=1.4.0 (from twscrape)\n",
      "  Downloading fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting httpx>=0.26.0 (from twscrape)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting loguru>=0.7.0 (from twscrape)\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /Users/taylory/.local/share/virtualenvs/cos350-a4-orb5bJtS/lib/python3.11/site-packages (from aiosqlite>=0.17.0->twscrape) (4.9.0)\n",
      "Collecting anyio (from httpx>=0.26.0->twscrape)\n",
      "  Downloading anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in /Users/taylory/.local/share/virtualenvs/cos350-a4-orb5bJtS/lib/python3.11/site-packages (from httpx>=0.26.0->twscrape) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx>=0.26.0->twscrape)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /Users/taylory/.local/share/virtualenvs/cos350-a4-orb5bJtS/lib/python3.11/site-packages (from httpx>=0.26.0->twscrape) (3.6)\n",
      "Collecting sniffio (from httpx>=0.26.0->twscrape)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.26.0->twscrape)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading twscrape-0.11.1-py3-none-any.whl (30 kB)\n",
      "Downloading aiosqlite-0.20.0-py3-none-any.whl (15 kB)\n",
      "Downloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: fake-useragent, sniffio, loguru, h11, aiosqlite, httpcore, anyio, httpx, twscrape\n",
      "Successfully installed aiosqlite-0.20.0 anyio-4.3.0 fake-useragent-1.5.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 loguru-0.7.2 sniffio-1.3.1 twscrape-0.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"pip install twscrape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List\n",
    "import twscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Scraping general tweet set for given year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_directories(keyword_eng: str, year: str):\n",
    "    dir_path = year if keyword_eng == \"\" else keyword_eng + \"_\" + year\n",
    "    os.mkdir(dir_path)\n",
    "\n",
    "    # single digit month\n",
    "    for i in range(1,10):\n",
    "        path = dir_path + \"/\" + \"0\" + str(i)\n",
    "        os.mkdir(path) \n",
    "\n",
    "    # double digit month\n",
    "    for j in range(10,13):\n",
    "        path = dir_path + \"/\" + str(j)\n",
    "        os.mkdir(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape randomly sampled tweets for 10 days in given month\n",
    "def scrape_month_sampled_tweets(year: str, months: List[str], days_in_month: int):\n",
    "    range_days = list(range(1,days_in_month+1))\n",
    "    range_times = list(range(0,24))\n",
    "\n",
    "    # sample 10 random days and times of day for each month\n",
    "    for m in months: \n",
    "        month_path = year + \"/\" + m\n",
    "        days = sorted(random.sample(range_days, k=10))   # random days of month (no replacement)\n",
    "        times = random.choices(range_times, k=10)   # random hours of day (replacement)\n",
    "        \n",
    "        # scrape tweets for the 10 picked days and times\n",
    "        for t in range(10):     \n",
    "            day = \"0\" + str(days[t]) if days[t] < 10 else str(days[t])\n",
    "            time = \"0\" + str(times[t]) if times[t] < 10 else str(times[t])\n",
    "            date_string = year + '-' + m + '-' + day\n",
    "            day_path = month_path + \"/\" + year + \"-\" + m + \"-\" + day + \".txt\"\n",
    "            command = 'twscrape search \"since:' + date_string + '_' + time + ':00:00_UTC until:' + date_string + '_' + time + ':59:59_UTC lang:ja\"　> ' + day_path + ' --limit=4500'\n",
    "            os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape random sample of tweets (general content or keyword-search) from given year\n",
    "def scrape_year_sampled_tweets(year: str):\n",
    "    # 28 day months\n",
    "    scrape_month_sampled_tweets(year,[\"02\"], 28)    # omit leap year 29th days for simplicity...?\n",
    "\n",
    "    # 30 day months\n",
    "    months_30 = [\"04\",\"06\",\"09\",\"11\"]\n",
    "    scrape_month_sampled_tweets(year,months_30, 30)\n",
    "\n",
    "    # 31 day months\n",
    "    months_31 = [\"01\",\"03\",\"05\",\"07\",\"08\",\"10\",\"12\"]\n",
    "    scrape_month_sampled_tweets(year,months_31, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_general_txt_files(year: str, keyword_eng=\"\"):\n",
    "  # concatenate .txt files into one file per month\n",
    "  for root, dirs, files in os.walk(\"./\" + year):\n",
    "      for name in dirs:\n",
    "        month_path = os.path.join(root, name)\n",
    "        os.system(\"cat \" + month_path + \"/*.txt > \" + year + \"/\" + name + \".txt\")\n",
    "  \n",
    "  # concatenate month .txt files into one file for the year\n",
    "  os.system(\"cat \" + year + \"/*.txt > \" + year + \"-all.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping general year sample datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last took 1790m\n",
    "scrape_year_sampled_tweets(\"2015\")\n",
    "concatenate_general_txt_files(\"2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 21:13:48.239 | INFO     | twscrape.accounts_pool:get_for_queue_or_wait:275 - No account available for queue \"SearchTimeline\". Next available at 21:25:28\n"
     ]
    }
   ],
   "source": [
    "scrape_year_sampled_tweets(\"2022\")\n",
    "concatenate_general_txt_files(\"2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Scraping minority keyword-related tweet set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape tweets containing keyword from every month of given year\n",
    "def scrape_keyword_month_tweets(year: str, months: List[str], days_in_month: int, keyword_jp: str, keyword_eng: str):\n",
    "    for m in months: \n",
    "        command = 'twscrape search \"' + keyword_jp + ' since:' + year + '-' + m + '-01_00:00:00_UTC until:' + year + '-' + m + '-' + str(days_in_month) + '_23:59:59_UTC lang:ja\"　> ' + keyword_eng + '_' + year + '/' + m + '.txt --limit=30000'\n",
    "        os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape all tweets containing keyword in given year\n",
    "def scrape_keyword_sampled_tweets(year: str, keyword_jp: str, keyword_eng: str):\n",
    "    os.mkdir(keyword_eng + \"_\" + year) \n",
    "\n",
    "    # 28 day months\n",
    "    scrape_keyword_month_tweets(year,[\"02\"], 28, keyword_jp, keyword_eng)    # omit leap year 29th days for simplicity...?\n",
    "\n",
    "    # 30 day months\n",
    "    months_30 = [\"04\",\"06\",\"09\",\"11\"]\n",
    "    scrape_keyword_month_tweets(year,months_30, 30, keyword_jp, keyword_eng)\n",
    "\n",
    "    # 31 day months\n",
    "    months_31 = [\"01\",\"03\",\"05\",\"07\",\"08\",\"10\",\"12\"]\n",
    "    scrape_keyword_month_tweets(year,months_31, 31, keyword_jp, keyword_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_keyword_txt_files(year: str, keyword_eng=\"\"):\n",
    "  # concatenate month .txt files into one file for the year\n",
    "  os.system(\"cat \" + keyword_eng + \"_\" + year + \"/\" + \"*.txt > \" + keyword_eng + \"_\" + year + \".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minority keyword-related datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zainichi korean\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"在日コリアン\", \"zainichi\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"zainichi\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"在日コリアン\", \"zainichi\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"zainichi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ainu\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"アイヌ\", \"ainu\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"ainu\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"アイヌ\", \"ainu\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"ainu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ryukyujin \n",
    "scrape_keyword_sampled_tweets(\"2022\", \"琉球人\", \"ryukyujin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"ryukyujin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"琉球人\", \"ryukyujin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"ryukyujin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not finished but didn't use\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"琉球\", \"ryukyu\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"ryukyu\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"琉球\", \"ryukyu\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"ryukyu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okinawajin\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"沖縄人\", \"okinawajin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"okinawajin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"沖縄人\", \"okinawajin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"okinawajin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# haafu\n",
    "# this one took around 100 min per month...\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"ハーフ\", \"haafu\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"haafu\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"ハーフ\", \"haafu\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"haafu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vietnam\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"ベトナム人\", \"vietnamjin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"vietnamjin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"ベトナム人\", \"vietnamjin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"vietnamjin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# philippines\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"フィリピン人\", \"philippinejin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"philippinejin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"フィリピン人\", \"philippinejin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"philippinejin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nepal\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"ネパール人\", \"nepaljin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"nepaljin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"ネパール人\", \"nepaljin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"nepaljin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indonesia\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"インドネシア人\", \"indonesiajin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"indonesiajin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"インドネシア人\", \"indonesiajin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"indonesiajin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zainichi kankokujin\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"在日韓国人\", \"zainichi_kankokujin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"zainichi_kankokujin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"在日韓国人\", \"zainichi_kankokujin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"zainichi_kankokujin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaijin [didn't do 2022; hold]\n",
    "scrape_keyword_sampled_tweets(\"2022\", \"外人\", \"gaijin\")\n",
    "concatenate_keyword_txt_files(\"2022\", \"gaijin\")\n",
    "\n",
    "scrape_keyword_sampled_tweets(\"2015\", \"外人\", \"gaijin\")\n",
    "concatenate_keyword_txt_files(\"2015\", \"gaijin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-hw276cHk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
